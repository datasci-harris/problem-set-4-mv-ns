---
title: "Problem Set 4 - Hospital Data"
author: "Mario Venegas and Neil Stein"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
import pandas as pd

pos2016 = pd.read_csv('pos2016.csv')
print(pos2016.head())
```

1. 

```{python}
print("Variable names:", pos2016.columns.tolist())
```

2. 
    a.
```{python} 
unique_values = pos2016['PRVDR_CTGRY_CD'].unique()
print("Unique values for PRVDR_CTGRY_CD:", unique_values)

unique_values = pos2016['PRVDR_CTGRY_SBTYP_CD'].unique()
print("Unique values for PRVDR_CTGRY_SBTYP_CD:", unique_values)
```

```{python} 
pos2016 = pos2016[pos2016['PRVDR_CTGRY_CD'] == 1]
pos2016 = pos2016[pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2016.head())
```

```{python}
num_observations = pos2016.shape[0]
print("Number of observations in pos2016:", num_observations)
```

    b. By comparing numebr of observations for 2016 to the ones for 2017, 2018, and 2019, one can observe that the number is low. It could be possible that this way of identifying providers started to be used in 2017. 

3. 

2017
```{python}
pos2017 = pd.read_csv('pos2017.csv')
print(pos2017.head())
```

```{python} 
pos2017 = pos2017[pos2017['PRVDR_CTGRY_CD'] == 1]
pos2017 = pos2017[pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2017.shape[0]
print("Number of observations in pos2017:", num_observations)
```

2018
```{python}
pos2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
```

```{python} 
pos2018 = pos2018[pos2018['PRVDR_CTGRY_CD'] == 1]
pos2018 = pos2018[pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2018.head())
```

```{python}
num_observations = pos2018.shape[0]
print("Number of observations in pos2018:", num_observations)
```

2019
```{python}
pos2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')
```

```{python} 
pos2019 = pos2019[pos2019['PRVDR_CTGRY_CD'] == 1]
pos2019 = pos2019[pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2019.shape[0]
print("Number of observations in pos2019:", num_observations)
```

Append
```{python}
pos2017['year'] = 2017
pos2018['year'] = 2018
pos2019['year'] = 2019

print(pos2017.head)
```

```{python}
combined_df = pd.concat([pos2017, pos2018, pos2019], axis=0, ignore_index=True)

print(combined_df.columns)
```

```{python}
import altair as alt
```

```{python}
observations_per_year = combined_df.groupby('year').size().reset_index(name='count')

chart = alt.Chart(observations_per_year).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count:Q', title='Number of Observations'),
    tooltip=['year:O', 'count:Q']
).properties(
    title='Number of Observations Per Year'
)

chart
```

4. 
    a.
```{python}
unique_prvdr_counts = combined_df.groupby('year')['PRVDR_NUM'].nunique().reset_index()

unique_prvdr_counts.columns = ['year', 'unique_prvdr_count']
```

```{python}
chart = alt.Chart(unique_prvdr_counts).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('unique_prvdr_count:Q', title='Unique Number of Hospitals'),
    tooltip=['year:O', 'unique_prvdr_count:Q']
).properties(
    title='Unique Number of Hospitals Per Year'
)
chart
```

    b.In 2019, the way of identifying hospitals changed. So this variable was not reported for any hospital this year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
import pandas as pd

# finding suspect closures - joining data

all_years_df = pd.merge(pos2016, combined_df, on= "FAC_NAME")
closures_df = all_years_df.groupby(["FAC_NAME", "ZIP_CD_y"])["year"].max().reset_index()


# isolating closures
target_year = 2019
closed_early = closures_df[closures_df["year"] < target_year]
print(f"There are {len(closed_early)} hospitals fitting this description")
```

2. 

```{python}
import pandas as pd

# sorting and finding the top 10

closed_early_sorted = closed_early.sort_values(by= "FAC_NAME")
print(closed_early_sorted[["FAC_NAME","year"]].head(10))
```

3. 
    a.
```{python}
import pandas as pd

# function to comb through and check total hospitals by zip across years
def merger_checker(df):
    grouped_df = df.groupby(["ZIP_CD", "year", "FAC_NAME", "PRVDR_NUM"]).size().reset_index(name= "count") 
    grouped_df = grouped_df.sort_values(["ZIP_CD", "year"])
    grouped_df["merger_suspect"] = grouped_df.groupby("ZIP_CD")["count"].diff() !=0
    grouped_df["merger_suspect"] = grouped_df["merger_suspect"].fillna(False)
    return grouped_df

merger_flagged_df = merger_checker(combined_df)
'''
print(merger_flagged_df.head(10))
'''

# checked the function via print, looks good! now we filter
merge_filter = merger_flagged_df[merger_flagged_df["merger_suspect"] == True]

```

    b.
 ```{python}
import pandas as pd

# handling empty/NA values
merge_filter["PRVDR_NUM"] = merge_filter["PRVDR_NUM"].fillna("terminated")
print(f"There are {sum(merge_filter["PRVDR_NUM"] == "terminated")} hospitals fitting this description")

# correcting to subset out the terminated codes
terminated_cleaned = merge_filter[merge_filter["PRVDR_NUM"] != "terminated"]
print(f"There are {len(terminated_cleaned)} hospitals that are left fitting this description")
 ```

    c.
```{python}
import pandas as pd

# sorting by name
terminated_cleaned = terminated_cleaned.sort_values(by= "FAC_NAME")
print(terminated_cleaned["FAC_NAME"].head(10))
```


## Download Census zip code shapefile (10 pt) 

1. 
    a.These files are associated with a geographic information system.
    - .shp (shapefile) contains geometric data like points, lines, and polygos. 
    - .shx (Shape Indez File) it is an intex for the .shp file, meaning it helps locate the geometry of the shapes stored in the .shp file. 
    - .dbf ( Database File) stores attribute data for features defined in .shp file, such as names and measurements. Each file in .shp is a row in .dbf.
    - .prj (Projection File) contains coordinate systems and projections used by the shapefile.
    - .html (HTML file) contains documentation about the dataset or the shapefile.

    b. The shape file is the biggest by far 817,915 KB, followed by .dbg by around 6,000, and then the .shx by 215 KB. The smallest of them all is .prj with just 1 KB.
2. 

```{python}
import geopandas as gpd

zipcodes = gpd.read_file('gz_2010_us_860_00_500k.shp')

print(zipcodes.head)
```

```{python}
# Filter for Texas zip codes
texas_zipcodes = zipcodes[zipcodes['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

texas_zipcodes['ZCTA5'].nunique
```

```{python}
# Filter for the year 2016 and count hospitals per zip code
df_2016_tx = pos2016
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].astype(str)
```

```{python}
import pandas as pd

df_2016_tx = df_2016_tx[
    (df_2016_tx['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79'))) &
    (df_2016_tx['ZIP_CD'].str.len() == 7)
]

# Replace '.0' with an empty string in the ZIP_CD column
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].str.replace('.0', '', regex=False)

df_2016_tx = df_2016_tx.groupby("ZIP_CD").size().reset_index()

# Renaming the columns
df_2016_tx.columns = ['ZCTA5', 'HOSPITAL_COUNT']
```

```{python}
df_2016_tx = texas_zipcodes.merge(df_2016_tx, on="ZCTA5", how="left")

df_2016_tx['HOSPITAL_COUNT'] = df_2016_tx['HOSPITAL_COUNT'].fillna(0)
```


```{python}
# Plot
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(10, 10))

# Plot the Texas zip codes with the number of hospitals
df_2016_tx.plot(column='HOSPITAL_COUNT', cmap='Grays',
                    linewidth=0.7, ax=ax, edgecolor="0.6", legend=True)

ax.set_title("Hospitals in TX by Zipcode")

# Show the plot
plt.show()
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
import geopandas as gpd
import shapely
from shapely import Polygon, Point, LineString

# fresh import + getting the names of the columns
all_zipcodes = gpd.read_file("gz_2010_us_860_00_500k.shp")
all_zip_headers = all_zipcodes.columns
print(all_zip_headers)
print(all_zipcodes.head(10))

# centroid creation
all_zipcodes["centroid"] = all_zipcodes["geometry"].centroid
all_zipcodes["centroid"].centroid.plot(markersize= 1)
plt.axis("off")
plt.show()

# measuring our geo-dataframe
zips_all_centroids = all_zipcodes.copy()
length_all_zips = len(all_zipcodes)
width_all_zips = len(all_zip_headers)

print(f"the dimensions of zips_all_centroids is {length_all_zips} rows by {width_all_zips} columns")
```

# Columns Explained:
    GEO_ID - unique ID code combining zip code, state, and census area ID codes
    ZCTA5 - the 5 number zip code
    NAME - the name of the zip code, in this case identical to ZCTA5
    LSAD - Legal/Statistical Area Description, string to describe the geometry's category of statistical area 
    CENSUSAREA - describes the specific census area involved
    geometry - the polygonal attribute data, the coordinate dimensions of the polygon (or multi-polygon for certain areas) that are the borders of the statistical area

2. 

```{python}
import geopandas as gpd

# Texas subset of zipcodes
zips_texas_centroids = all_zipcodes[all_zipcodes["ZCTA5"].str.startswith(("75","76","77","78", "79","88"))]
# neighboring states plus texas -- asked ChatGPT to give me "what are the two digits at the start of the zipcodes for states surrounding Texas"
texas_border_zips = ["75","76","77","78","79","88","70","71","72","73","74","87","88"]
zips_texas_borderstates_centroids = all_zipcodes[all_zipcodes["ZCTA5"].str.startswith(tuple(texas_border_zips))]

# counting unique zips
tx_unique = zips_texas_centroids["ZCTA5"].nunique()
tx_border_unique = zips_texas_borderstates_centroids["ZCTA5"].nunique()
print(f"there are {tx_unique} zipcodes in Texas and {tx_border_unique} zipcodes in bordering states + Texas")
```

Please note - the suggested approach in the prompt would yield the number of zipcodes in Texas and only one interesting polygon in the bordering state, not the total in the bordering state, which is a logical problem in the prompt. I will execute the instructions as listed in the more accurate method listed above, however below is that code which the prompt suggests as being the right approach (again, not actually correct per the wording of the prompt). There is no column in the "gz_2010_us_860_00_500k.shp" data set that codes for State, so this prompt suggestion can't work without outside data.

```{python}
'''
import geopandas as pd
import uszipcode
from shapely.geometry import shape

# data prep
tx_poly = zips_texas_centroids.dissolve()
tx_zip_prefixes = ["75","76","77","78", "79","88"]

non_tx_zips = all_zipcodes[~all_zipcodes["ZCTA5"].astype(str).str[:2].isin(tx_zip_prefixes)]
spat_index = non_tx_zips.sindex

# function creation
def poly_intersect(row, tx_poly):
    bordered_poly = sidx.query(tx_poly.bounds)
    for idx in bordered_poly:
        if row["geometry"].intersects(tx_poly.geometry[0]):
            return True
        return False

non_tx_zips["intersects_tx"] = non_tx_zips.apply(poly_intersect, axis=1, args=(tx_poly,))

# as mentioned, without state data we are stuck - solution is to import a new package that Chat GPT recommended - uszipcode

search = uszipcode.SearchEngine(db= "zipcode")
non_tx_zips["State"] = non_tx_zips["ZCTA5"].apply(lambda x: search.by_zipcode(x).state)
bordering_states = non_tx_zips[non_tx_zips["intersects_tx"]]["State"].unique()

print(bordering_states)
'''
```

3. 
```{python}
import geopandas as gpd

# Joining the data (inner) and filtering where at least one hospital is open in 2016
joint_16_tx = zips_texas_borderstates_centroids.merge(df_2016_tx, on= "ZCTA5", how= "inner")
zips_withhospital_centroids = joint_16_tx.groupby("ZCTA5").filter(lambda x: len(x) >=1)

print(f"In 2016, there were {len(zips_withhospital_centroids)} zipcodes with at least one hospital")
```

Explanation - We decided on an inner join to combine this data, as we are interested in where we have intersecting occurances. Since earlier we cleaned up the ZCTA5 column in the texas 2016 df we decided to merge on that variable, which is also present in zips_texas_borderstates_centroids

4. 
    a.
```{python}
import geopandas as gpd
from shapely import Point

# subset of 10 rows for the test
ten_zip_test = zips_withhospital_centroids.head(10)

# testing the larger code burden with these ten
zips_texas_borderstates_centroids.set_geometry("centroid", inplace=True)
ten_zip_test.set_geometry("centroid", inplace=True)
spatial_test = gpd.sjoin_nearest(zips_texas_borderstates_centroids, ten_zip_test, how= "left", distance_col= "distance_m")

# adding in distance calulcations
def distance_calculator(row):
    centroid1 = row["centroid"]
    centroid2 = ten_zip_test.loc[row["index_right"], "centroid"]
    return centroid1.distance(centroid2)

spatial_test["distance_meters"] = spatial_test.apply(distance_calculator, axis=1)

print(spatial_test[["ZCTA5_left", "ZCTA5_right", "distance_meters"]])
```

This result works, it ran on my computer in about 7 seconds (6.7 seconds specifically). Given that the length of the zips_withhospital_centroids full GeoDataFrame is 1935 in length, we can estimate this will take about 20 minutes to do the full dataframe!

    b.
```{python}
import geopandas as gpd
from shapely import Point
# full data setup
zips_texas_borderstates_centroids.set_geometry("centroid", inplace=True)
zips_withhospital_centroids.set_geometry("centroid", inplace=True)

spatial_join = gpd.sjoin_nearest(zips_texas_borderstates_centroids, zips_withhospital_centroids, how= "left", distance_col= "distance_m")

# remove identical zips (no distance between the same centroids!)
spatial_join = spatial_join[spatial_join["ZCTA5_left"] != spatial_join["ZCTA5_right"]]
spatial_join["distance_km"] = spatial_join["distance_m"] / 1000

```

    c.
```{python}
# .prj file is in meters, I earlier converted to km, so now to switch to miles
spatial_join["distance_miles"] = spatial_join["distance_m"] * 0.000621371
```

5. 
```{python}
zips_withhospital_centroids.plot() 
plt.show()

```

```{python}
import pandas as pd
import geopandas as gpd

# distance calculation
average_distances = spatial_join.groupby("ZCTA5_left")["distance_miles"].mean().reset_index()

print(f"the average distance is {average_distances["distance_miles"].mean():.2f} miles")

# setting up plotting
hospital_dist_plotting = zips_withhospital_centroids.merge(average_distances, left_on= "ZCTA5", right_on= "ZCTA5_left", how= "left")

# Plotting our values
hospital_dist_plotting.plot(column= "distance_miles", legend= True)
plt.title( "Average Distance to Nearest Hospital (Miles)")
plt.show()
```

The average value reported here is 0.00 miles, which does not make much sense. This might suggest that there is a hospital in every zipcode, however that does not seem particularly likely given the results of section 3
    
## Effects of closures on access in Texas (15 pts)

1. 
```{python}
import pandas as pd

# Ensure 'ZIP_CD' is of string type
closed_early['ZIP_CD'] = closed_early['ZIP_CD'].astype(str)

# Filter for Texas zip codes
closed_early_tx = closed_early[
    closed_early['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79')) &
    (closed_early['ZIP_CD'].str.len() == 7)
]

# Replace '.0' with an empty string in the ZIP_CD column
closed_early_tx['ZIP_CD'] = closed_early_tx['ZIP_CD'].str.replace('.0', '', regex=False)

# Group by ZIP_CD and count the number of closures
closed_early_tx = closed_early_tx.groupby("ZIP_CD").size().reset_index(name='HOSPITAL_COUNT')

# Rename columns
closed_early_tx.columns = ['ZCTA5', 'HOSPITAL_COUNT']

print(closed_early_tx)
```

2. 
```{python}
closed_early_tx = texas_zipcodes.merge(closed_early_tx, on="ZCTA5", how="left")

closed_early_tx['HOSPITAL_COUNT'] = closed_early_tx['HOSPITAL_COUNT'].fillna(0)

print(closed_early_tx.columns)
```

```{python}
# Plot
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(10, 10))

# Plot the Texas zip codes with the number of hospitals
closed_early_tx.plot(column='HOSPITAL_COUNT', cmap='Grays',
                    linewidth=0.7, ax=ax, edgecolor="0.6", legend=True)

ax.set_title("Closed hospitals in TX")

plt.show()
```

3.

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

affected_zipcodes_gdf = texas_zipcodes[texas_zipcodes['ZCTA5'].isin(closed_early_tx['ZCTA5'])]

affected_zipcodes_gdf["buffer"] = affected_zipcodes_gdf.geometry.buffer(16093.4)

affected_zipcodes_gdf = affected_zipcodes_gdf.rename(columns={"ZCTA5": "ZCTA5_buffer"})

indirectly_affected = gpd.sjoin(
    texas_zipcodes, affected_zipcodes_gdf, how="inner", predicate="intersects")

buffer_gdf = indirectly_affected["ZCTA5_buffer"].unique()

indirectly_affected_zips = indirectly_affected["ZCTA5"].unique()

indirectly_affected_zips = indirectly_affected_zips[~np.isin(
    indirectly_affected_zips, buffer_gdf)]

count = len(indirectly_affected_zips)

print(f"Number of indirectly affected zip codes: {count}")
```

4. 

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

# Rename columns for consistency
affected_zipcodes_gdf = affected_zipcodes_gdf.rename(columns={'ZCTA5_buffer': 'ZCTA5'})
indirectly_affected_zipcodes = indirectly_affected_zipcodes.rename(columns={'ZCTA5_left': 'ZCTA5'})

# Create sets for affected zip codes
directly_affected_set = set(affected_zipcodes_gdf['ZCTA5']) 
indirectly_affected_set = set(indirectly_affected_zipcodes['ZCTA5']) 

# Create a set for not affected zip codes
all_texas_zipcodes = set(texas_zipcodes['ZCTA5'])
not_affected_set = all_texas_zipcodes - directly_affected_set - indirectly_affected_set


def categorize_zipcodes(row):
    if row['ZCTA5'] in directly_affected_set:
        return 'Directly Affected'
    elif row['ZCTA5'] in indirectly_affected_set:
        return 'Indirectly Affected'
    elif row['ZCTA5'] in new_data_set:
        return 'Not Affected'  # Considered as not affected if in new_data
    else:
        return 'Not Affected'

# Add a new column for categories
texas_zipcodes['category'] = texas_zipcodes.apply(categorize_zipcodes, axis=1)

# Debug: Check the distribution of categories
category_counts = texas_zipcodes['category'].value_counts()
print("Category Distribution:\n", category_counts)

# Define color mapping
color_map = {
    'Directly Affected': 'orange',
    'Indirectly Affected': 'red',
    'Not Affected': 'green'
}

# Map categories to colors
texas_zipcodes['color'] = texas_zipcodes['category'].map(color_map)

# Plotting
fig, ax = plt.subplots(figsize=(10, 10))
texas_zipcodes.plot(color=texas_zipcodes['color'], ax=ax, edgecolor='black')
ax.set_title("Texas Zip Codes Affected by Hospital Closures")
plt.show()

```

## Reflecting on the exercise (10 pts) 

Partner 1- 
- Data Inconsistencies: Different sources may report hospital closures at varying times, leading to discrepancies. Changes in ownership or management can also affect closure statuses, making it challenging to maintain accurate records.

- Incomplete Reporting: Hospitals, especially in rural areas, may not report closures promptly or accurately. This can result in missing data for facilities that have ceased operations but have not been officially documented as closed.

- Temporary Closures Confusion: Some hospitals may temporarily close for renovations or other reasons, which can be misclassified as permanent closures. Clear definitions are needed to differentiate between temporary and permanent status to ensure accurate reporting.

Partner 2 - The way we are measuring closures, by their presence in year n-1 and lack of presence in year n, is not the best way to track this information. We have identified in this exercise many hospitals that fit under this definition, yet we are not clearly tracking closures, rather the end of that particular hospital code being used. I believe that beyond the identified reason of mergers, there could be plenty of other reasons, such as downsizing, that have hospitals change their status. In my working experience in rural America I know that the difference between a rural medical care facility being a hospital vs. a clinic is often a matter of personnel, and people can easily shift around, which would result in a hospital being de-listed here as this list does not cover every single medical care facility. A better method here would be to pull out a list of names from our suspected closures list and perform a quick web-scraping operation to see whether or not this location is still open!
