---
title: "Problem Set 4 - Hospital Data"
author: "Mario Venegas and Neil Stein"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

```{python}
import pandas as pd

pos2016 = pd.read_csv('pos2016.csv')
print(pos2016.head())
```

1. 

```{python}
print("Variable names:", pos2016.columns.tolist())
```

2. 
    a.
```{python} 
unique_values = pos2016['PRVDR_CTGRY_CD'].unique()
print("Unique values for PRVDR_CTGRY_CD:", unique_values)

unique_values = pos2016['PRVDR_CTGRY_SBTYP_CD'].unique()
print("Unique values for PRVDR_CTGRY_SBTYP_CD:", unique_values)
```

```{python} 
pos2016 = pos2016[pos2016['PRVDR_CTGRY_CD'] == 1]
pos2016 = pos2016[pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2016.head())
```

```{python}
num_observations = pos2016.shape[0]
print("Number of observations in pos2016:", num_observations)
```

    b. By comparing numebr of observations for 2016 to the ones for 2017, 2018, and 2019, one can observe that the number is low. It could be possible that this way of identifying providers started to be used in 2017. 

3. 

2017
```{python}
pos2017 = pd.read_csv('pos2017.csv')
print(pos2017.head())
```

```{python} 
pos2017 = pos2017[pos2017['PRVDR_CTGRY_CD'] == 1]
pos2017 = pos2017[pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2017.shape[0]
print("Number of observations in pos2017:", num_observations)
```

2018
```{python}
pos2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
```

```{python} 
pos2018 = pos2018[pos2018['PRVDR_CTGRY_CD'] == 1]
pos2018 = pos2018[pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2018.head())
```

```{python}
num_observations = pos2018.shape[0]
print("Number of observations in pos2018:", num_observations)
```

2019
```{python}
pos2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')
```

```{python} 
pos2019 = pos2019[pos2019['PRVDR_CTGRY_CD'] == 1]
pos2019 = pos2019[pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1.0]
print(pos2017.head())
```

```{python}
num_observations = pos2019.shape[0]
print("Number of observations in pos2019:", num_observations)
```

Append
```{python}
pos2017['year'] = 2017
pos2018['year'] = 2018
pos2019['year'] = 2019

print(pos2017.head)
```

```{python}
combined_df = pd.concat([pos2017, pos2018, pos2019], axis=0, ignore_index=True)

print(combined_df.columns)
```

```{python}
import altair as alt
```

```{python}
observations_per_year = combined_df.groupby('year').size().reset_index(name='count')

chart = alt.Chart(observations_per_year).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('count:Q', title='Number of Observations'),
    tooltip=['year:O', 'count:Q']
).properties(
    title='Number of Observations Per Year'
)

chart
```

4. 
    a.
```{python}
unique_prvdr_counts = combined_df.groupby('year')['PRVDR_NUM'].nunique().reset_index()

unique_prvdr_counts.columns = ['year', 'unique_prvdr_count']
```

```{python}
chart = alt.Chart(unique_prvdr_counts).mark_bar().encode(
    x=alt.X('year:O', title='Year'),
    y=alt.Y('unique_prvdr_count:Q', title='Unique Number of Hospitals'),
    tooltip=['year:O', 'unique_prvdr_count:Q']
).properties(
    title='Unique Number of Hospitals Per Year'
)
chart
```

    b.In 2019, the way of identifying hospitals changed. So this variable was not reported for any hospital this year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
import pandas as pd

# finding suspect closures - joining data

all_years_df = pd.merge(pos2016, combined_df, on= "FAC_NAME")
closures_df = all_years_df.groupby(["FAC_NAME", "ZIP_CD_y"])["year"].max().reset_index()


# isolating closures
target_year = 2019
closed_early = closures_df[closures_df["year"] < target_year]
print(f"There are {len(closed_early)} hospitals fitting this description")
```

2. 

```{python}
import pandas as pd

# sorting and finding the top 10

closed_early_sorted = closed_early.sort_values(by= "FAC_NAME")
print(closed_early_sorted[["FAC_NAME","year"]].head(10))
```

3. 
    a.
```{python}
import pandas as pd

# function to comb through and check total hospitals by zip across years
def merger_checker(df):
    grouped_df = df.groupby(["ZIP_CD", "year", "FAC_NAME", "PRVDR_NUM"]).size().reset_index(name= "count") 
    grouped_df = grouped_df.sort_values(["ZIP_CD", "year"])
    grouped_df["merger_suspect"] = grouped_df.groupby("ZIP_CD")["count"].diff() !=0
    grouped_df["merger_suspect"] = grouped_df["merger_suspect"].fillna(False)
    return grouped_df

merger_flagged_df = merger_checker(combined_df)
'''
print(merger_flagged_df.head(10))
'''

# checked the function via print, looks good! now we filter
merge_filter = merger_flagged_df[merger_flagged_df["merger_suspect"] == True]

```

    b.
 ```{python}
import pandas as pd

# handling empty/NA values
merge_filter["PRVDR_NUM"] = merge_filter["PRVDR_NUM"].fillna("terminated")
print(f"There are {sum(merge_filter["PRVDR_NUM"] == "terminated")} hospitals fitting this description")

# correcting to subset out the terminated codes
terminated_cleaned = merge_filter[merge_filter["PRVDR_NUM"] != "terminated"]
print(f"There are {len(terminated_cleaned)} hospitals that are left fitting this description")
 ```

    c.
```{python}
import pandas as pd

# sorting by name
terminated_cleaned = terminated_cleaned.sort_values(by= "FAC_NAME")
print(terminated_cleaned["FAC_NAME"].head(10))
```


## Download Census zip code shapefile (10 pt) 

1. 
    a.These files are associated with a geographic information system.
    - .shp (shapefile) contains geometric data like points, lines, and polygos. 
    - .shx (Shape Indez File) it is an intex for the .shp file, meaning it helps locate the geometry of the shapes stored in the .shp file. 
    - .dbf ( Database File) stores attribute data for features defined in .shp file, such as names and measurements. Each file in .shp is a row in .dbf.
    - .prj (Projection File) contains coordinate systems and projections used by the shapefile.
    - .html (HTML file) contains documentation about the dataset or the shapefile.

    b. The shape file is the biggest by far 817,915 KB, followed by .dbg by around 6,000, and then the .shx by 215 KB. The smallest of them all is .prj with just 1 KB.
2. 

```{python}
import geopandas as gpd

zipcodes = gpd.read_file('gz_2010_us_860_00_500k.shp')

print(zipcodes.head)
```

```{python}
# Filter for Texas zip codes
texas_zipcodes = zipcodes[zipcodes['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

texas_zipcodes['ZCTA5'].nunique
```

```{python}
# Filter for the year 2016 and count hospitals per zip code
df_2016_tx = pos2016
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].astype(str)
```

```{python}
import pandas as pd

df_2016_tx = df_2016_tx[
    (df_2016_tx['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79'))) &
    (df_2016_tx['ZIP_CD'].str.len() == 7)
]

# Replace '.0' with an empty string in the ZIP_CD column
df_2016_tx['ZIP_CD'] = df_2016_tx['ZIP_CD'].str.replace('.0', '', regex=False)

df_2016_tx = df_2016_tx.groupby("ZIP_CD").size().reset_index()

# Renaming the columns
df_2016_tx.columns = ['ZCTA5', 'HOSPITAL_COUNT']
```

```{python}
df_2016_tx = texas_zipcodes.merge(df_2016_tx, on="ZCTA5", how="left")

df_2016_tx['HOSPITAL_COUNT'] = df_2016_tx['HOSPITAL_COUNT'].fillna(0)
```


```{python}
# Plot
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(10, 10))

# Plot the Texas zip codes with the number of hospitals
df_2016_tx.plot(column='HOSPITAL_COUNT', cmap='Grays',
                    linewidth=0.7, ax=ax, edgecolor="0.6", legend=True)

ax.set_title("Hospitals in TX by Zipcode")
